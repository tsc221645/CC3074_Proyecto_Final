{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fase 1 del proyecto\n",
    "\n",
    "En esta primera fase se limpiará los datos y se elegirá una problemática para brindar solución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDownload necessary libraries if not have them already\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Download necessary libraries if not have them already\n",
    "'''\n",
    "# !pip install numpy\n",
    "# !pip install matplotlib.pyplot\n",
    "# !pip install seaborn\n",
    "# !pip install scikit-learn \n",
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache deleted: C:\\Users\\josue/.cache/kagglehub/datasets/olistbr/brazilian-ecommerce\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/olistbr/brazilian-ecommerce?dataset_version_number=2...\n",
      "Extracting files...\n",
      "File moved: olist_customers_dataset.csv\n",
      "File moved: olist_geolocation_dataset.csv\n",
      "File moved: olist_orders_dataset.csv\n",
      "File moved: olist_order_items_dataset.csv\n",
      "File moved: olist_order_payments_dataset.csv\n",
      "File moved: olist_order_reviews_dataset.csv\n",
      "File moved: olist_products_dataset.csv\n",
      "File moved: olist_sellers_dataset.csv\n",
      "File moved: product_category_name_translation.csv\n",
      "Dataset successfully moved to: 'd:\\UVG GitHub Repositorios\\2025\\CC3074_Proyecto_Final\\data'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/42.6M [00:00<?, ?B/s]\n",
      "  2%|▏         | 1.00M/42.6M [00:00<00:10, 4.02MB/s]\n",
      " 12%|█▏        | 5.00M/42.6M [00:00<00:02, 17.4MB/s]\n",
      " 30%|███       | 13.0M/42.6M [00:00<00:00, 34.5MB/s]\n",
      " 52%|█████▏    | 22.0M/42.6M [00:00<00:00, 49.8MB/s]\n",
      " 68%|██████▊   | 29.0M/42.6M [00:00<00:00, 55.8MB/s]\n",
      " 84%|████████▍ | 36.0M/42.6M [00:00<00:00, 59.1MB/s]\n",
      "100%|██████████| 42.6M/42.6M [00:00<00:00, 61.5MB/s]\n",
      "100%|██████████| 42.6M/42.6M [00:00<00:00, 47.5MB/s]\n"
     ]
    }
   ],
   "source": [
    "!python ../scripts/download_dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploración de datos\n",
    "Primero se va a chequear si existe datos nulos en los csv y recabar la cantidad de datos nullos encontrados por cada .csv del conjunto de datos. De igual forma\n",
    "se verá qué tipos de datos se maneja y ver cantidatos para aplicar transformación a datos enteros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Callable, List, Tuple, Set\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these functions will help with the data analysis \n",
    "# there's chance to add more information if deemeded necessary\n",
    "def get_null_count(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Gives a count of nulls per column and its relative percentage.\n",
    "    It does not change or replace nulls.\n",
    "    params: \n",
    "        df: dataframe to get null info of\n",
    "    returns:\n",
    "        summary: dataframe that has a null count per col and a relative % of nulls in that col\n",
    "    '''\n",
    "    nulls: pd.Series = df.isnull().sum() # get the count of nulls per col\n",
    "    percent: pd.Series = (nulls / len(df)) * 100 # get the percentage of nulls \n",
    "    summary: pd.DataFrame = pd.DataFrame({\n",
    "        \"Nulls\" : nulls,\n",
    "        \"Percentage\" : percent.round(2)\n",
    "    })\n",
    "    return summary[summary['Nulls'] > 0].sort_values(by='Nulls', ascending=False) # sort values by Nulls col , can change ascending to true\n",
    "\n",
    "def get_types(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Gives the datatypes found within a dataframe.\n",
    "    It does not change datatypes.\n",
    "    params: \n",
    "        df: dataframe to get types of\n",
    "    returns:\n",
    "        types: dataframe with the type of each column\n",
    "    '''\n",
    "    types: pd.DataFrame = pd.DataFrame({\n",
    "        'col_name' : df.columns,\n",
    "        'data_type' : df.dtypes.astype(str).values\n",
    "    })\n",
    "    return types\n",
    "\n",
    "def get_row_and_col_count(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Gives the number of rows and columns found in a dataframe\n",
    "    params:\n",
    "        df: dataframe to get num of cols and rows\n",
    "    returns:\n",
    "        num_row_col: dataframe with the number of columns and rows\n",
    "    '''\n",
    "    num_row_col: pd.DataFrame = pd.DataFrame([{\n",
    "        'num_cols': len(df.columns),\n",
    "        'num_rows': len(df)\n",
    "    }])\n",
    "    return num_row_col\n",
    "\n",
    "def get_descriptive_stats(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Gives a brief description of the dataframe\n",
    "    '''\n",
    "    return df.describe().transpose()\n",
    "\n",
    "def get_unique(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Return unique values found in the dataframe\n",
    "    params:\n",
    "        df: dataframe to get unique values from\n",
    "    return:\n",
    "        dataframe with unique values\n",
    "    '''\n",
    "    return df.nunique().to_frame(name='Unique Values')\n",
    "\n",
    "def get_top_frequent(df: pd.DataFrame, top_n: int = 3) -> pd.DataFrame:\n",
    "    '''\n",
    "    Returns top 10 most frequent values found in the dataframe\n",
    "    params: \n",
    "        df: dataframe to get unique values\n",
    "        top_n: top number of values to get\n",
    "    '''\n",
    "    rows:list = []\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        top_vals = df[col].value_counts().head(top_n) # get the top 3 values\n",
    "        for val, freq in top_vals.items(): # from those values, get the top value and its frequency\n",
    "            rows.append({\n",
    "                'Column Name': col,\n",
    "                'Top Value': str(val), \n",
    "                'Frequency': freq\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def write_report(df: pd.DataFrame, df_name: str, path: str, summary_functions: List[Callable]) -> None:\n",
    "    '''\n",
    "    Method that writes a report into a .txt of a certain dataframe\n",
    "    params: \n",
    "        df: dataframe to be read\n",
    "        path: where the .txt log will be saved\n",
    "        summary_functions: a list functions that will be run and its returns be saved into the report\n",
    "    returns:\n",
    "        None\n",
    "    '''\n",
    "    if summary_functions is None:\n",
    "        print(\"Please provide functions to be able to write a summary\")\n",
    "    \n",
    "    with open(path, 'w') as log:\n",
    "        log.write(f\"DataFrame Summary Report Of {df_name}\\n\") # write header\n",
    "        log.write(\"=\" * 80 + \"\\n\\n\") \n",
    "\n",
    "        # for each function in the summary_functions list, write its summary and results\n",
    "        for func in summary_functions:\n",
    "            section_title: str = func.__name__.replace('_', ' ').title() # get the function name as title\n",
    "            log.write(f\"{section_title}\\n\")\n",
    "            log.write(\"-\" * len(section_title) + \"\\n\")\n",
    "            result_df: pd.DataFrame = func(df) # pass the dataframe argument to the function\n",
    "            if result_df.empty:\n",
    "                log.write(\"No information to report\\n\\n\")\n",
    "            else:\n",
    "                log.write(result_df.to_string(index=True))\n",
    "                log.write(\"\\n\\n\")\n",
    "        \n",
    "        log.write(\"=\" * 80 + \"\\nEnd\\n\") # finish the report\n",
    "\n",
    "\n",
    "def generate_report_folder(csv_folder: str, report_folder: str, summary_functions: List[Callable]) -> None:\n",
    "    '''\n",
    "    Generates a report folder based upon the functions\n",
    "    params:\n",
    "        csv_folder: where the csv files are \n",
    "        report_folder: final folder where reports will be stored\n",
    "        summary_functions: functions' returns that will be stored in the report\n",
    "    '''\n",
    "    os.makedirs(report_folder, exist_ok=True) # create folder if it does not exist\n",
    "    csv_files: list = glob.glob(os.path.join(csv_folder, '*.csv'))\n",
    "    for csv_path in csv_files:\n",
    "        try:\n",
    "            df: pd.DataFrame = pd.read_csv(csv_path)\n",
    "            file_name: str = os.path.splitext(os.path.basename(csv_path))[0]\n",
    "            report_path: str = os.path.join(report_folder, f\"{file_name}_raw_report.txt\")\n",
    "            write_report(df=df, df_name=file_name, path=report_path, summary_functions=summary_functions)\n",
    "            print(f\"Report written into: {report_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed processing {csv_path}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report written into: ../dataset_reports_raw\\olist_customers_dataset_raw_report.txt\n",
      "Report written into: ../dataset_reports_raw\\olist_geolocation_dataset_raw_report.txt\n",
      "Report written into: ../dataset_reports_raw\\olist_orders_dataset_raw_report.txt\n",
      "Report written into: ../dataset_reports_raw\\olist_order_items_dataset_raw_report.txt\n",
      "Report written into: ../dataset_reports_raw\\olist_order_payments_dataset_raw_report.txt\n",
      "Report written into: ../dataset_reports_raw\\olist_order_reviews_dataset_raw_report.txt\n",
      "Report written into: ../dataset_reports_raw\\olist_products_dataset_raw_report.txt\n",
      "Report written into: ../dataset_reports_raw\\olist_sellers_dataset_raw_report.txt\n",
      "Report written into: ../dataset_reports_raw\\product_category_name_translation_raw_report.txt\n"
     ]
    }
   ],
   "source": [
    "# these reports are only based on each individual .csv, not on the database as a whole\n",
    "generate_report_folder(csv_folder=\"../data\", report_folder=\"../dataset_reports_raw\", summary_functions=[get_null_count, get_types, get_row_and_col_count, get_descriptive_stats, \n",
    "                                                                                                           get_top_frequent, get_unique])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformación de datos\n",
    "Al observar que hay ciertos conjuntos de datos con varios datos faltantes como en olist_order_reviews. Donde casi un 80% de las reseñas están vacías. Hay otras que también están vacías pero son cercanas a menos del 5%. Por lo que podemos decidir prescindir de estas sin problema alguno. Sin embargo, debemos considerar que esos porcentajes son locales en esos .csv, no son absolutos. Lo que quiere decir es que, no sabemos si los nulos de un csv son los mismos nulos de otro csv. Por ejemplo:\n",
    "\n",
    "Si tenemos csv1 y csv2, asociados mediante un customer_id, y el csv1 tiene 4% de nulos y el csv2 tiene 2%, no sabemos si para el customer_id los nulos del csv1 y csv2 asociados a esa llave serán los mismos.\n",
    "Por lo que tenemos que estar seguros que combinando la cantidad de nulos hallados en ambos, el total no sea mayor a 5%. Porque si los datos asociados al 4% de nulos son diferentes a los datos del 2% de nulos, estaríamos borrando un 6% de datos en general. \n",
    "\n",
    "\n",
    "Ahora bien, por lo mismo, al tener los datos normalizados en distintos .csv tenemos que considerar que si se elimina un dato de un .csv, se debe eliminar sus registros asociados en todos los .csv. De lo contrario, esto dará problemas a la hora de intentar modelar ya que el conjunto de datos estará incompleto o con datos faltantes en ciertos registros.\n",
    "\n",
    "Entonces tenemos la elección de, o manejar los datos en el procesamiento de forma normalizada, o desnormalizar los datos y convegerlos en un solo conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO not been tested yet\n",
    "def delete_nulls(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Deletes nulls from a dataframe if the count is less than 5%\n",
    "    '''\n",
    "    null: pd.DataFrame = get_null_count(df) \n",
    "    to_clean: list = null[null['Percentage'] > 5].index.tolist()\n",
    "    if to_clean:\n",
    "        return df.dropna(subset=to_clean)\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "#TODO possible approach to clean the dataset\n",
    "def denormalize_dataset(dfs: List[pd.DataFrame]) -> pd.DataFrame:\n",
    "    '''\n",
    "    Denormalizes the datasets into one big dataset\n",
    "    Input:\n",
    "        dfs: a list of the dataframes to denormalize\n",
    "    Return:\n",
    "        a denormalized dataframe\n",
    "    '''\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis estadístico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
