{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fase 1 del proyecto\n",
    "\n",
    "En esta primera fase se limpiará los datos y se elegirá una problemática para brindar solución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in c:\\users\\lpmon\\documents\\github\\cc3074_proyecto_final\\.venv\\lib\\site-packages (1.15.2)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in c:\\users\\lpmon\\documents\\github\\cc3074_proyecto_final\\.venv\\lib\\site-packages (from scipy) (2.2.4)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Download necessary libraries if not have them already\n",
    "'''\n",
    "# !pip install numpy\n",
    "# !pip install matplotlib.pyplot\n",
    "# !pip install seaborn\n",
    "# !pip install scikit-learn \n",
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploración de datos\n",
    "Primero se va a chequear si existe datos nulos en los csv y recabar la cantidad de datos nullos encontrados por cada .csv del conjunto de datos. De igual forma\n",
    "se verá qué tipos de datos se maneja y ver cantidatos para aplicar transformación a datos enteros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Callable, List, Tuple\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these functions will help with the data analysis \n",
    "# there's chance to add more information if deemeded necessary\n",
    "def get_null_count(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Gives a count of nulls per column and its relative percentage.\n",
    "    It does not change or replace nulls.\n",
    "    params: \n",
    "        df: dataframe to get null info of\n",
    "    returns:\n",
    "        summary: dataframe that has a null count per col and a relative % of nulls in that col\n",
    "    '''\n",
    "    nulls: pd.Series = df.isnull().sum() # get the count of nulls per col\n",
    "    percent: pd.Series = (nulls / len(df)) * 100 # get the percentage of nulls \n",
    "    summary: pd.DataFrame = pd.DataFrame({\n",
    "        \"Nulls\" : nulls,\n",
    "        \"Percentage\" : percent.round(2)\n",
    "    })\n",
    "    return summary[summary['Nulls'] > 0].sort_values(by='Nulls', ascending=False) # sort values by Nulls col , can change ascending to true\n",
    "\n",
    "def get_types(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Gives the datatypes found within a dataframe.\n",
    "    It does not change datatypes.\n",
    "    params: \n",
    "        df: dataframe to get types of\n",
    "    returns:\n",
    "        types: dataframe with the type of each column\n",
    "    '''\n",
    "    types: pd.DataFrame = pd.DataFrame({\n",
    "        'col_name' : df.columns,\n",
    "        'data_type' : df.dtypes.astype(str).values\n",
    "    })\n",
    "    return types\n",
    "\n",
    "def get_row_and_col_count(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Gives the number of rows and columns found in a dataframe\n",
    "    params:\n",
    "        df: dataframe to get num of cols and rows\n",
    "    returns:\n",
    "        num_row_col: dataframe with the number of columns and rows\n",
    "    '''\n",
    "    num_row_col: pd.DataFrame = pd.DataFrame([{\n",
    "        'num_cols': len(df.columns),\n",
    "        'num_rows': len(df)\n",
    "    }])\n",
    "    return num_row_col\n",
    "\n",
    "def get_descriptive_stats(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Gives a brief description of the dataframe\n",
    "    '''\n",
    "    return df.describe().transpose()\n",
    "\n",
    "def get_unique(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Return unique values found in the dataframe\n",
    "    params:\n",
    "        df: dataframe to get unique values from\n",
    "    return:\n",
    "        dataframe with unique values\n",
    "    '''\n",
    "    return df.nunique().to_frame(name='Unique Values')\n",
    "\n",
    "def get_top_frequent(df: pd.DataFrame, top_n: int = 3) -> pd.DataFrame:\n",
    "    '''\n",
    "    Returns top 10 most frequent values found in the dataframe\n",
    "    params: \n",
    "        df: dataframe to get unique values\n",
    "        top_n: top number of values to get\n",
    "    '''\n",
    "    rows:list = []\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        top_vals = df[col].value_counts().head(top_n) # get the top 3 values\n",
    "        for val, freq in top_vals.items(): # from those values, get the top value and its frequency\n",
    "            rows.append({\n",
    "                'Column Name': col,\n",
    "                'Top Value': str(val), \n",
    "                'Frequency': freq\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def write_report(df: pd.DataFrame, df_name: str, path: str, summary_functions: List[Callable]) -> None:\n",
    "    '''\n",
    "    Method that writes a report into a .txt of a certain dataframe\n",
    "    params: \n",
    "        df: dataframe to be read\n",
    "        path: where the .txt log will be saved\n",
    "        summary_functions: a list functions that will be run and its returns be saved into the report\n",
    "    returns:\n",
    "        None\n",
    "    '''\n",
    "    if summary_functions is None:\n",
    "        print(\"Please provide functions to be able to write a summary\")\n",
    "    \n",
    "    with open(path, 'w') as log:\n",
    "        log.write(f\"DataFrame Summary Report Of {df_name}\\n\") # write header\n",
    "        log.write(\"=\" * 80 + \"\\n\\n\") \n",
    "\n",
    "        # for each function in the summary_functions list, write its summary and results\n",
    "        for func in summary_functions:\n",
    "            section_title: str = func.__name__.replace('_', ' ').title() # get the function name as title\n",
    "            log.write(f\"{section_title}\\n\")\n",
    "            log.write(\"-\" * len(section_title) + \"\\n\")\n",
    "            result_df: pd.DataFrame = func(df) # pass the dataframe argument to the function\n",
    "            if result_df.empty:\n",
    "                log.write(\"No information to report\\n\\n\")\n",
    "            else:\n",
    "                log.write(result_df.to_string(index=True))\n",
    "                log.write(\"\\n\\n\")\n",
    "        \n",
    "        log.write(\"=\" * 80 + \"\\nEnd\\n\") # finish the report\n",
    "\n",
    "\n",
    "def generate_report_folder(csv_folder: str, report_folder: str, summary_functions: List[Callable]) -> None:\n",
    "    '''\n",
    "    Generates a report folder based upon the functions\n",
    "    params:\n",
    "        csv_folder: where the csv files are \n",
    "        report_folder: final folder where reports will be stored\n",
    "        summary_functions: functions' returns that will be stored in the report\n",
    "    '''\n",
    "    os.makedirs(report_folder, exist_ok=True) # create folder if it does not exist\n",
    "    csv_files: list = glob.glob(os.path.join(csv_folder, '*.csv'))\n",
    "    for csv_path in csv_files:\n",
    "        try:\n",
    "            df: pd.DataFrame = pd.read_csv(csv_path)\n",
    "            file_name: str = os.path.splitext(os.path.basename(csv_path))[0]\n",
    "            report_path: str = os.path.join(report_folder, f\"{file_name}_raw_report.txt\")\n",
    "            write_report(df=df, df_name=file_name, path=report_path, summary_functions=summary_functions)\n",
    "            print(f\"Report written into: {report_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed processing {csv_path}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report written into: ../dataset_reports_raw\\olist_customers_dataset_raw_report.txt\n",
      "Report written into: ../dataset_reports_raw\\olist_geolocation_dataset_raw_report.txt\n",
      "Report written into: ../dataset_reports_raw\\olist_orders_dataset_raw_report.txt\n",
      "Report written into: ../dataset_reports_raw\\olist_order_items_dataset_raw_report.txt\n",
      "Report written into: ../dataset_reports_raw\\olist_order_payments_dataset_raw_report.txt\n",
      "Report written into: ../dataset_reports_raw\\olist_order_reviews_dataset_raw_report.txt\n",
      "Report written into: ../dataset_reports_raw\\olist_products_dataset_raw_report.txt\n",
      "Report written into: ../dataset_reports_raw\\olist_sellers_dataset_raw_report.txt\n",
      "Report written into: ../dataset_reports_raw\\product_category_name_translation_raw_report.txt\n"
     ]
    }
   ],
   "source": [
    "generate_report_folder(csv_folder=\"../dataset\", report_folder=\"../dataset_reports_raw\", summary_functions=[get_null_count, get_types, get_row_and_col_count, get_descriptive_stats, \n",
    "                                                                                                           get_top_frequent, get_unique])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis estadístico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformación de datos\n",
    "Al observar que hay ciertos conjuntos de datos con varios datos faltantes como en olist_order_reviews. Donde casi un 80% de las reseñas están vacías. Hay otras que también están vacías pero son cercanas a menos del 5%. Por lo que podemos decidir prescindir de estas o no. Ahora bien, debemos ver si realmente utilizaremos los reviews del dataset para nuestro fin o si no. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
