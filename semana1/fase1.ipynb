{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fase 1 del proyecto\n",
    "\n",
    "En esta primera fase se limpiará los datos y se elegirá una problemática para brindar solución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDownload necessary libraries if not have them already\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Download necessary libraries if not have them already\n",
    "'''\n",
    "# !pip install numpy\n",
    "# !pip install matplotlib.pyplot\n",
    "# !pip install seaborn\n",
    "# !pip install scikit-learn \n",
    "# !pip install pandas\n",
    "# %pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache deleted: C:\\Users\\josue/.cache/kagglehub/datasets/olistbr/brazilian-ecommerce\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/olistbr/brazilian-ecommerce?dataset_version_number=2...\n",
      "Extracting files...\n",
      "File moved: olist_customers_dataset.csv\n",
      "File moved: olist_geolocation_dataset.csv\n",
      "File moved: olist_orders_dataset.csv\n",
      "File moved: olist_order_items_dataset.csv\n",
      "File moved: olist_order_payments_dataset.csv\n",
      "File moved: olist_order_reviews_dataset.csv\n",
      "File moved: olist_products_dataset.csv\n",
      "File moved: olist_sellers_dataset.csv\n",
      "File moved: product_category_name_translation.csv\n",
      "Dataset successfully moved to: 'd:\\UVG GitHub Repositorios\\2025\\CC3074_Proyecto_Final\\data'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/42.6M [00:00<?, ?B/s]\n",
      "  2%|▏         | 1.00M/42.6M [00:00<00:10, 4.16MB/s]\n",
      " 12%|█▏        | 5.00M/42.6M [00:00<00:02, 17.7MB/s]\n",
      " 28%|██▊       | 12.0M/42.6M [00:00<00:00, 33.2MB/s]\n",
      " 42%|████▏     | 18.0M/42.6M [00:00<00:00, 41.9MB/s]\n",
      " 63%|██████▎   | 27.0M/42.6M [00:00<00:00, 57.6MB/s]\n",
      " 84%|████████▍ | 36.0M/42.6M [00:00<00:00, 67.5MB/s]\n",
      "100%|██████████| 42.6M/42.6M [00:00<00:00, 51.3MB/s]\n"
     ]
    }
   ],
   "source": [
    "!python ../scripts/download_dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploración de datos\n",
    "Primero se va a chequear si existe datos nulos en los csv y recabar la cantidad de datos nullos encontrados por cada .csv del conjunto de datos. De igual forma\n",
    "se verá qué tipos de datos se maneja y ver cantidatos para aplicar transformación a datos enteros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Callable, List, Tuple, Set\n",
    "import os\n",
    "import glob\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these functions will help with the data analysis \n",
    "# there's chance to add more information if deemeded necessary\n",
    "def get_null_count(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Gives a count of nulls per column and its relative percentage.\n",
    "    It does not change or replace nulls.\n",
    "    params: \n",
    "        df: dataframe to get null info of\n",
    "    returns:\n",
    "        summary: dataframe that has a null count per col and a relative % of nulls in that col\n",
    "    '''\n",
    "    nulls: pd.Series = df.isnull().sum() # get the count of nulls per col\n",
    "    percent: pd.Series = (nulls / len(df)) * 100 # get the percentage of nulls \n",
    "    summary: pd.DataFrame = pd.DataFrame({\n",
    "        \"Nulls\" : nulls,\n",
    "        \"Percentage\" : percent.round(2)\n",
    "    })\n",
    "    return summary[summary['Nulls'] > 0].sort_values(by='Nulls', ascending=False) # sort values by Nulls col , can change ascending to true\n",
    "\n",
    "def get_types(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Gives the datatypes found within a dataframe.\n",
    "    It does not change datatypes.\n",
    "    params: \n",
    "        df: dataframe to get types of\n",
    "    returns:\n",
    "        types: dataframe with the type of each column\n",
    "    '''\n",
    "    types: pd.DataFrame = pd.DataFrame({\n",
    "        'col_name' : df.columns,\n",
    "        'data_type' : df.dtypes.astype(str).values\n",
    "    })\n",
    "    return types\n",
    "\n",
    "def get_row_and_col_count(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Gives the number of rows and columns found in a dataframe\n",
    "    params:\n",
    "        df: dataframe to get num of cols and rows\n",
    "    returns:\n",
    "        num_row_col: dataframe with the number of columns and rows\n",
    "    '''\n",
    "    num_row_col: pd.DataFrame = pd.DataFrame([{\n",
    "        'num_cols': len(df.columns),\n",
    "        'num_rows': len(df)\n",
    "    }])\n",
    "    return num_row_col\n",
    "\n",
    "def get_descriptive_stats(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Gives a brief description of the dataframe\n",
    "    '''\n",
    "    return df.describe().transpose()\n",
    "\n",
    "def get_unique(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Return unique values found in the dataframe\n",
    "    params:\n",
    "        df: dataframe to get unique values from\n",
    "    return:\n",
    "        dataframe with unique values\n",
    "    '''\n",
    "    return df.nunique().to_frame(name='Unique Values')\n",
    "\n",
    "def get_top_frequent(df: pd.DataFrame, top_n: int = 3) -> pd.DataFrame:\n",
    "    '''\n",
    "    Returns top 10 most frequent values found in the dataframe\n",
    "    params: \n",
    "        df: dataframe to get unique values\n",
    "        top_n: top number of values to get\n",
    "    '''\n",
    "    rows:list = []\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        top_vals = df[col].value_counts().head(top_n) # get the top 3 values\n",
    "        for val, freq in top_vals.items(): # from those values, get the top value and its frequency\n",
    "            rows.append({\n",
    "                'Column Name': col,\n",
    "                'Top Value': str(val), \n",
    "                'Frequency': freq\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def write_report(df: pd.DataFrame, df_name: str, path: str, summary_functions: List[Callable]) -> None:\n",
    "    '''\n",
    "    Method that writes a report into a .txt of a certain dataframe\n",
    "    params: \n",
    "        df: dataframe to be read\n",
    "        path: where the .txt log will be saved\n",
    "        summary_functions: a list functions that will be run and its returns be saved into the report\n",
    "    returns:\n",
    "        None\n",
    "    '''\n",
    "    if summary_functions is None:\n",
    "        print(\"Please provide functions to be able to write a summary\")\n",
    "    \n",
    "    with open(path, 'w') as log:\n",
    "        log.write(f\"DataFrame Summary Report Of {df_name}\\n\") # write header\n",
    "        log.write(\"=\" * 80 + \"\\n\\n\") \n",
    "\n",
    "        # for each function in the summary_functions list, write its summary and results\n",
    "        for func in summary_functions:\n",
    "            section_title: str = func.__name__.replace('_', ' ').title() # get the function name as title\n",
    "            log.write(f\"{section_title}\\n\")\n",
    "            log.write(\"-\" * len(section_title) + \"\\n\")\n",
    "            result_df: pd.DataFrame = func(df) # pass the dataframe argument to the function\n",
    "            if result_df.empty:\n",
    "                log.write(\"No information to report\\n\\n\")\n",
    "            else:\n",
    "                log.write(result_df.to_string(index=True))\n",
    "                log.write(\"\\n\\n\")\n",
    "        \n",
    "        log.write(\"=\" * 80 + \"\\nEnd\\n\") # finish the report\n",
    "\n",
    "\n",
    "def generate_report_folder(csv_folder: str, report_folder: str, summary_functions: List[Callable]) -> None:\n",
    "    '''\n",
    "    Generates a report folder based upon the functions\n",
    "    params:\n",
    "        csv_folder: where the csv files are \n",
    "        report_folder: final folder where reports will be stored\n",
    "        summary_functions: functions' returns that will be stored in the report\n",
    "    '''\n",
    "    os.makedirs(report_folder, exist_ok=True) # create folder if it does not exist\n",
    "    csv_files: list = glob.glob(os.path.join(csv_folder, '*.csv'))\n",
    "    for csv_path in csv_files:\n",
    "        try:\n",
    "            df: pd.DataFrame = pd.read_csv(csv_path)\n",
    "            file_name: str = os.path.splitext(os.path.basename(csv_path))[0]\n",
    "            report_path: str = os.path.join(report_folder, f\"{file_name}_raw_report.txt\")\n",
    "            write_report(df=df, df_name=file_name, path=report_path, summary_functions=summary_functions)\n",
    "            print(f\"Report written into: {report_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed processing {csv_path}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these reports are only based on each individual .csv, not on the database as a whole\n",
    "generate_report_folder(csv_folder=\"../dataset_raw\", report_folder=\"../dataset_reports_raw\", summary_functions=[get_null_count, get_types, get_row_and_col_count, get_descriptive_stats, \n",
    "                                                                                                           get_top_frequent, get_unique])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformación de datos\n",
    "Al observar que hay ciertos conjuntos de datos con varios datos faltantes como en olist_order_reviews. Donde casi un 80% de las reseñas están vacías. Hay otras que también están vacías pero son cercanas a menos del 5%. Por lo que podemos decidir prescindir de estas sin problema alguno. Sin embargo, debemos considerar que esos porcentajes son locales en esos .csv, no son absolutos. Lo que quiere decir es que, no sabemos si los nulos de un csv son los mismos nulos de otro csv. Por ejemplo:\n",
    "\n",
    "Si tenemos csv1 y csv2, asociados mediante un customer_id, y el csv1 tiene 4% de nulos y el csv2 tiene 2%, no sabemos si para el customer_id los nulos del csv1 y csv2 asociados a esa llave serán los mismos.\n",
    "Por lo que tenemos que estar seguros que combinando la cantidad de nulos hallados en ambos, el total no sea mayor a 5%. Porque si los datos asociados al 4% de nulos son diferentes a los datos del 2% de nulos, estaríamos borrando un 6% de datos en general. \n",
    "\n",
    "\n",
    "Ahora bien, por lo mismo, al tener los datos normalizados en distintos .csv tenemos que considerar que si se elimina un dato de un .csv, se debe eliminar sus registros asociados en todos los .csv. De lo contrario, esto dará problemas a la hora de intentar modelar ya que el conjunto de datos estará incompleto o con datos faltantes en ciertos registros.\n",
    "\n",
    "Entonces tenemos la elección de, o manejar los datos en el procesamiento de forma normalizada, o desnormalizar los datos y convegerlos en un solo conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO not been tested yet\n",
    "def delete_nulls(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Deletes nulls from a dataframe if the count is less than 5%\n",
    "    '''\n",
    "    null: pd.DataFrame = get_null_count(df) \n",
    "    to_clean: list = null[null['Percentage'] > 5].index.tolist()\n",
    "    if to_clean:\n",
    "        return df.dropna(subset=to_clean)\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "#TODO possible approach to clean the dataset\n",
    "def denormalize_dataset(dfs: List[pd.DataFrame]) -> pd.DataFrame:\n",
    "    '''\n",
    "    Denormalizes the datasets into one big dataset\n",
    "    Input:\n",
    "        dfs: a list of the dataframes to denormalize\n",
    "    Return:\n",
    "        a denormalized dataframe\n",
    "    '''\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza e imputación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "orders = pd.read_csv(\"../data/olist_orders_dataset.csv\", parse_dates=[\n",
    "    \"order_purchase_timestamp\", \"order_approved_at\",\n",
    "    \"order_delivered_carrier_date\", \"order_delivered_customer_date\",\n",
    "    \"order_estimated_delivery_date\"\n",
    "])\n",
    "\n",
    "reviews = pd.read_csv(\"../data/olist_order_reviews_dataset.csv\")\n",
    "products = pd.read_csv(\"../data/olist_products_dataset.csv\")\n",
    "customers = pd.read_csv(\"../data/olist_customers_dataset.csv\")\n",
    "geolocation = pd.read_csv(\"../data/olist_geolocation_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza de nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Se eliminan ordenes sin fechas de entrega al cliente.\n",
    "orders = orders[orders['order_delivered_customer_date'].notnull()]\n",
    "\n",
    "# 2. Imputación de fechas utilizando la media\n",
    "orders['order_delivered_carrier_date'] = orders['order_delivered_carrier_date'].fillna(\n",
    "    orders['order_delivered_carrier_date'].median()\n",
    ")\n",
    "orders['order_approved_at'] = orders['order_approved_at'].fillna(\n",
    "    orders['order_purchase_timestamp']\n",
    ")\n",
    "\n",
    "# 3. Flags para eomcentarios\n",
    "reviews['has_title'] = reviews['review_comment_title'].notnull().astype(int)\n",
    "reviews['has_message'] = reviews['review_comment_message'].notnull().astype(int)\n",
    "\n",
    "# 4. Imputar nulos con mediana o texto para los productos\n",
    "products['product_category_name'] = products['product_category_name'].fillna(\"unknown\")\n",
    "num_cols = [\n",
    "    'product_name_lenght', 'product_description_lenght', 'product_photos_qty',\n",
    "    'product_weight_g', 'product_length_cm', 'product_height_cm', 'product_width_cm'\n",
    "]\n",
    "for col in num_cols:\n",
    "    products[col] = products[col].fillna(products[col].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea la variable target dificultad logística para poder clasificar el tretraso en tres niveles: fácil, moderada y difícil. Esto permite un mejor manejo de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders['delivery_delay'] = (\n",
    "    (orders['order_delivered_customer_date'] - orders['order_estimated_delivery_date']).dt.days\n",
    ")\n",
    "orders['dificultad_logistica'] = pd.cut(\n",
    "    orders['delivery_delay'],\n",
    "    bins=[-np.inf, 0, 5, np.inf],\n",
    "    labels=[\"fácil\", \"moderada\", \"difícil\"]\n",
    ")\n",
    "orders['delivery_time'] = (\n",
    "    orders['order_delivered_customer_date'] - orders['order_purchase_timestamp']\n",
    ").dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unificación de datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utilizacon las llaves para realizar los joins entre orders, reviews, order_items, etc. Esto con el objetivo de que cada fila del dataset contenga toda la información relevante para poder predecir la dificultad de la entrega."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Orders + Customers\n",
    "df = orders.merge(customers, on='customer_id', how='left')\n",
    "\n",
    "# 2. Orders + Reviews \n",
    "reviews_reduced = reviews[['order_id', 'review_score', 'has_title', 'has_message']]\n",
    "df = df.merge(reviews_reduced, on='order_id', how='left')\n",
    "\n",
    "# 3. Order Items \n",
    "order_items = pd.read_csv(\"../data/olist_order_items_dataset.csv\")\n",
    "df = df.merge(order_items[['order_id', 'product_id', 'freight_value', 'price']], on='order_id', how='left')\n",
    "\n",
    "# 4. Products\n",
    "df = df.merge(products, on='product_id', how='left')\n",
    "\n",
    "#.5 Payments\n",
    "payments = pd.read_csv(\"../data/olist_order_payments_dataset.csv\")\n",
    "payment_summary = payments.groupby(\"order_id\").agg({\n",
    "    \"payment_type\": lambda x: x.mode()[0] if not x.mode().empty else \"unknown\"\n",
    "}).reset_index()\n",
    "df = df.merge(payment_summary, on=\"order_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se aplica One-Hot Encoding a variables categóricas como product_category_name y customer_state para convertir el texto a formato numérico. Las variables numéricas como freight_value y price se escalaron con StandarScaler para evitar la influencia de las diferencias de escala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['payment_type'] = df['payment_type'].fillna('unknown')  # si lo usás\n",
    "df['product_category_name'] = df['product_category_name'].astype(str)\n",
    "\n",
    "categorical_cols = ['customer_state', 'product_category_name']\n",
    "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "numerical_cols = ['freight_value', 'price', 'product_weight_g', 'delivery_time']\n",
    "scaler = StandardScaler()\n",
    "df[numerical_cols] = scaler.fit_transform(df[numerical_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separación entre variables predictorias y variable objetivo, eliminando nulos y columnas irrelevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>order_status</th>\n",
       "      <th>order_purchase_timestamp</th>\n",
       "      <th>order_approved_at</th>\n",
       "      <th>order_delivered_carrier_date</th>\n",
       "      <th>order_delivered_customer_date</th>\n",
       "      <th>order_estimated_delivery_date</th>\n",
       "      <th>delivery_delay</th>\n",
       "      <th>dificultad_logistica</th>\n",
       "      <th>...</th>\n",
       "      <th>product_category_name_portateis_casa_forno_e_cafe</th>\n",
       "      <th>product_category_name_portateis_cozinha_e_preparadores_de_alimentos</th>\n",
       "      <th>product_category_name_relogios_presentes</th>\n",
       "      <th>product_category_name_seguros_e_servicos</th>\n",
       "      <th>product_category_name_sinalizacao_e_seguranca</th>\n",
       "      <th>product_category_name_tablets_impressao_imagem</th>\n",
       "      <th>product_category_name_telefonia</th>\n",
       "      <th>product_category_name_telefonia_fixa</th>\n",
       "      <th>product_category_name_unknown</th>\n",
       "      <th>product_category_name_utilidades_domesticas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e481f51cbdc54678b7cc49136f2d6af7</td>\n",
       "      <td>9ef432eb6251297304e76186b10a928d</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-10-02 10:56:33</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>2017-10-04 19:55:00</td>\n",
       "      <td>2017-10-10 21:25:13</td>\n",
       "      <td>2017-10-18</td>\n",
       "      <td>-8</td>\n",
       "      <td>fácil</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53cdb2fc8bc7dce0b6741e2150273451</td>\n",
       "      <td>b0830fb4747a6c6d20dea0b8c802d7ef</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-07-24 20:41:37</td>\n",
       "      <td>2018-07-26 03:24:27</td>\n",
       "      <td>2018-07-26 14:31:00</td>\n",
       "      <td>2018-08-07 15:27:45</td>\n",
       "      <td>2018-08-13</td>\n",
       "      <td>-6</td>\n",
       "      <td>fácil</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47770eb9100c2d0c44946d9cf07ec65d</td>\n",
       "      <td>41ce2a54c0b03bf3443c3d931a367089</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-08-08 08:38:49</td>\n",
       "      <td>2018-08-08 08:55:23</td>\n",
       "      <td>2018-08-08 13:50:00</td>\n",
       "      <td>2018-08-17 18:06:29</td>\n",
       "      <td>2018-09-04</td>\n",
       "      <td>-18</td>\n",
       "      <td>fácil</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>949d5b44dbf5de918fe9c16f97b45f8a</td>\n",
       "      <td>f88197465ea7920adcdbec7375364d82</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-11-18 19:28:06</td>\n",
       "      <td>2017-11-18 19:45:59</td>\n",
       "      <td>2017-11-22 13:39:59</td>\n",
       "      <td>2017-12-02 00:28:42</td>\n",
       "      <td>2017-12-15</td>\n",
       "      <td>-13</td>\n",
       "      <td>fácil</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ad21c59c0840e6cb83a9ceb5573f8159</td>\n",
       "      <td>8ab97904e6daea8866dbdbc4fb7aad2c</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-02-13 21:18:39</td>\n",
       "      <td>2018-02-13 22:20:29</td>\n",
       "      <td>2018-02-14 19:46:34</td>\n",
       "      <td>2018-02-16 18:17:02</td>\n",
       "      <td>2018-02-26</td>\n",
       "      <td>-10</td>\n",
       "      <td>fácil</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 127 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           order_id                       customer_id  \\\n",
       "0  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
       "1  53cdb2fc8bc7dce0b6741e2150273451  b0830fb4747a6c6d20dea0b8c802d7ef   \n",
       "2  47770eb9100c2d0c44946d9cf07ec65d  41ce2a54c0b03bf3443c3d931a367089   \n",
       "3  949d5b44dbf5de918fe9c16f97b45f8a  f88197465ea7920adcdbec7375364d82   \n",
       "4  ad21c59c0840e6cb83a9ceb5573f8159  8ab97904e6daea8866dbdbc4fb7aad2c   \n",
       "\n",
       "  order_status order_purchase_timestamp   order_approved_at  \\\n",
       "0    delivered      2017-10-02 10:56:33 2017-10-02 11:07:15   \n",
       "1    delivered      2018-07-24 20:41:37 2018-07-26 03:24:27   \n",
       "2    delivered      2018-08-08 08:38:49 2018-08-08 08:55:23   \n",
       "3    delivered      2017-11-18 19:28:06 2017-11-18 19:45:59   \n",
       "4    delivered      2018-02-13 21:18:39 2018-02-13 22:20:29   \n",
       "\n",
       "  order_delivered_carrier_date order_delivered_customer_date  \\\n",
       "0          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
       "1          2018-07-26 14:31:00           2018-08-07 15:27:45   \n",
       "2          2018-08-08 13:50:00           2018-08-17 18:06:29   \n",
       "3          2017-11-22 13:39:59           2017-12-02 00:28:42   \n",
       "4          2018-02-14 19:46:34           2018-02-16 18:17:02   \n",
       "\n",
       "  order_estimated_delivery_date  delivery_delay dificultad_logistica  ...  \\\n",
       "0                    2017-10-18              -8                fácil  ...   \n",
       "1                    2018-08-13              -6                fácil  ...   \n",
       "2                    2018-09-04             -18                fácil  ...   \n",
       "3                    2017-12-15             -13                fácil  ...   \n",
       "4                    2018-02-26             -10                fácil  ...   \n",
       "\n",
       "   product_category_name_portateis_casa_forno_e_cafe  \\\n",
       "0                                              False   \n",
       "1                                              False   \n",
       "2                                              False   \n",
       "3                                              False   \n",
       "4                                              False   \n",
       "\n",
       "  product_category_name_portateis_cozinha_e_preparadores_de_alimentos  \\\n",
       "0                                              False                    \n",
       "1                                              False                    \n",
       "2                                              False                    \n",
       "3                                              False                    \n",
       "4                                              False                    \n",
       "\n",
       "   product_category_name_relogios_presentes  \\\n",
       "0                                     False   \n",
       "1                                     False   \n",
       "2                                     False   \n",
       "3                                     False   \n",
       "4                                     False   \n",
       "\n",
       "  product_category_name_seguros_e_servicos  \\\n",
       "0                                    False   \n",
       "1                                    False   \n",
       "2                                    False   \n",
       "3                                    False   \n",
       "4                                    False   \n",
       "\n",
       "   product_category_name_sinalizacao_e_seguranca  \\\n",
       "0                                          False   \n",
       "1                                          False   \n",
       "2                                          False   \n",
       "3                                          False   \n",
       "4                                          False   \n",
       "\n",
       "   product_category_name_tablets_impressao_imagem  \\\n",
       "0                                           False   \n",
       "1                                           False   \n",
       "2                                           False   \n",
       "3                                           False   \n",
       "4                                           False   \n",
       "\n",
       "   product_category_name_telefonia product_category_name_telefonia_fixa  \\\n",
       "0                            False                                False   \n",
       "1                            False                                False   \n",
       "2                            False                                False   \n",
       "3                            False                                False   \n",
       "4                            False                                False   \n",
       "\n",
       "   product_category_name_unknown  product_category_name_utilidades_domesticas  \n",
       "0                          False                                         True  \n",
       "1                          False                                        False  \n",
       "2                          False                                        False  \n",
       "3                          False                                        False  \n",
       "4                          False                                        False  \n",
       "\n",
       "[5 rows x 127 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['dificultad_logistica'].notnull()]\n",
    "\n",
    "X = df.drop(columns=['dificultad_logistica', 'order_id', 'customer_id', 'product_id'])\n",
    "y = df['dificultad_logistica']\n",
    "\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
